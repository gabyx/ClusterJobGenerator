{
  "name": "HPCJobConfigurator",
  "tagline": "Python module to generate highly configurable user-defined jobs on a high-performance cluster",
  "body": "# Job Configurator for Cluster Computing\r\n## Python module to generate highly configurable user-defined jobs for high-performance computing.\r\n\r\nIf you are doing high performance computing on a linux cluster and are bored from dealing with tons of configurations files for maintaining a parallel job configuration.\r\nThis python module has been developed during research for granular rigid body dynamics where complex and huge parallel MPI jobs needed to be configured in an uniform an reproducable way by using mainly one configuration file which acts as the main config file for the jobs [Link](http://www.zfm.ethz.ch/~nuetzig/?page=research). That included parallel tasks such as visualization, e.g rendering, general rigid body simulations, data analysis and image correlation on the HPC Euler and Brutus at ETH Zürich.\r\n\r\nThe absolut main advantage of this configurator is the ability to configure and test the job locally on your preferred workstation. The job can be fully launched locally and tested until everything is in place and works as expected and once this is the case it can be configured and submitted to the batch system on the cluster where it should complete sucessfully as well. (75% of the time :v:)\r\n\r\n## Activate the HPCJobConfigurator\r\nTo activate the macros in your current shell run\r\n```bash\r\nsource ./activate\r\n```\r\n\r\nThis module is best described with an example:\r\n## General Workflow\r\nThe user sets up a job configuration folder ``myJob`` consiting of two files ``Launch.ini`` and ``JobConfig.ini``.\r\n\r\n  - **The ``Launch.ini``** is a config file for all command line arguments to the configurator script ``configureJob.py``. It is not so important but handy.\r\n  - **The ``JobConfig.ini``** is **the main** job configuration file which is used for the job generator type specified at the command line to ``configureJob.py`` or in the ``Launch.ini``.\r\n\r\nBy using the ``configureJob.py`` script, the user configures the job (or a sequence of jobs) and the configuration files are written commonly to a job specific configuration folder\r\n``myJob/Launch_myJob.0/`` (or ``myJob/Launch_myJob.0/``,``myJob/Launch_myJob.1/``,... for a sequence of job configurations). What configuration files are written is dependent on the used type of job generator. The job generator is specified in the ``Launch.ini``.  **The job generator is a python class which is loaded and executed.**\r\n\r\n### The JobGeneratatorMPI\r\nThe main job generator for HPC tasks is the ``JobGeneratorMPI`` which generates a general launchable MPI task which consists of the following main Bash files executed in the main execution file ``launch.sh`` :\r\n\r\n  - **[start.sh](https://github.com/gabyx/HPClusterJobConfigurator/blob/master/HPCJobConfigurator/jobGenerators/jobGeneratorMPI/templates/start.sh)** : The start script which makes a global output folder (for example on a parallel filesystem) and sends an email to the user to inform him that the job has been started\r\n  - **[preProcess.sh](https://github.com/gabyx/HPClusterJobConfigurator/blob/master/HPCJobConfigurator/jobGenerators/jobGeneratorMPI/templates/preProcess.sh)** The pre-process script is executed for each node on the cluser (by using ``mpirun`` in ``launch.sh``). This pre-process is responsible to setup node specific scratch folders and other stuff, such as copying files to the node locally etc.\r\n  - **[process.sh](https://github.com/gabyx/HPClusterJobConfigurator/blob/master/HPCJobConfigurator/jobGenerators/jobGeneratorMPI/templates/process.sh)** The main process which is executed for each process on the HPC (by using ``mpirun`` in ``launch.sh``)\r\n  - **[postProcess.sh](https://github.com/gabyx/HPClusterJobConfigurator/blob/master/HPCJobConfigurator/jobGenerators/jobGeneratorMPI/templates/postProcess.sh)** The post-process which does file copies and clean up stuff and runs once for each single node (by using ``mpirun`` in ``launch.sh``).\r\n  - **[end.sh](https://github.com/gabyx/HPClusterJobConfigurator/blob/master/jobGenerators/jobGeneratorMPI/templates/end.sh)** The overall final process which at the end sends the user an email that the job has finished.\r\n\r\nOf course,  special jobs need special handling and therefore these scripts are only a guid and be copied and modified\r\nfor your needs.\r\n\r\nLets look for example at the template ``start.sh``:\r\n\r\n```bash\r\nfunction currTime(){ date +\"%H:%M:%S.%3N\"; }\r\nfunction ES(){ echo \"$(currTime) :: start.sh: \"; }\r\n\r\nyell() { echo \"$0: $*\" >&2; }\r\ndie() { yell \"$*\"; exit 1 ; }\r\ntry() { \"$@\" || die \"cannot $*\"; }\r\n\r\nif [[ \"${Cluster:mailAddress}\" != \"\" ]] ;  then\r\n    echo \"EOM\" | mail -s \"Job: ${Job:jobName} has started\" \"${Cluster:mailAddress}\"\r\nfi\r\n\r\n#echo \"Make global dir ${Job:globalDir}\"\r\ntry mkdir -p \"${Job:globalDir}\"\r\n\r\nexit 0\r\n```\r\n\r\nAll ``${section:option}`` strings (template strings) are replaced by the specified values in ``Launch.ini`` and **especially** ``JobConfig.ini``. So when the jobs are configured the template strings are replaced and the file ``start.sh`` is by default written to the job folder.\r\n\r\n#### A simple example\r\nThe [simple](https://github.com/gabyx/HPClusterJobConfigurator/blob/master/example/simple/) example job which has the following folder structure\r\n\r\n```\r\n├── JobConfig.ini                # Job configuration .ini file\r\n├── Launch.ini                   # configureJob.py .ini file\r\n├── data\r\n│   └── DataVisSettings.json     # some data to be used to configure the templates\r\n├── scripts\r\n│   └── simpleExeConfigurator.py # the configurator\r\n└── templates                    # two template files which are configured\r\n    ├── Input1.xml\r\n    └── Input2.txt\r\n```\r\n\r\nThe [simple](https://github.com/gabyx/HPClusterJobConfigurator/blob/master/example/simple/) example job can be configured by\r\n\r\n```bash\r\nsource ./activate\r\ncd examples/simple\r\nexport MYGLOBALSCRATCH_DIR=\"$(pwd)/scratch/global\"\r\nexport MYLOCALSCRATCH_DIR=\"$(pwd)/scratch/local\"\r\nconfigJob -x JobConfig.ini\r\n```\r\n\r\nwhich configures one job under ``examples/simple/cluster/Launch_MyDataVisualization.0``.\r\n\r\nLets look at some extraction of the two ``.ini`` files above:\r\n\r\n**[Launch.ini](https://github.com/gabyx/HPClusterJobConfigurator/blob/master/example/simple/Launch.ini):**\r\n\r\n```Ini\r\n...\r\n[Cluster]\r\njobName = MyDataVisualization\r\nmailAddress = user@mail.com\r\n...\r\n```\r\n\r\n**[JobConfig.ini](https://github.com/gabyx/HPClusterJobConfigurator/blob/master/example/simple/JobConfig.ini):**\r\n\r\n```Ini\r\n...\r\n[Job]\r\nglobalDir            = ENV::MYGLOBALSCRATCH_DIR/${Cluster:jobName}/${Cluster:jobName}.${Job:jobIdx}\r\nlocalDir             = ENV::MYLOCALSCRATCH_DIR/${Cluster:jobName}.${Job:jobIdx}\r\n\r\njobName              = ${Cluster:jobName}.${Job:jobIdx}\r\nscriptDirName        = Launch_${Job:jobName}\r\nscriptDir            = ${Cluster:jobGeneratorOutputDir}/${Job:scriptDirName}\r\n\r\n\r\n[Templates]\r\nstartJob                = ${General:modulePathGenerators}/jobGenerators/jobGeneratorMPI/generatorToolPipeline/templates/start.sh\r\n# ....\r\n\r\n# other templates ......\r\n\r\nmyOtherFancyTemplate    = ${General:jobDir}/templates/input1.xml\r\n\r\nmyOtherFancyTemplate2   = { \"inputFile\" : \"${General:jobDir}/templates/input2.txt\" , \"configurator\" : { \"modulePath\" : \"${General:modulePathConfigurator}/jobGenerators/dictionaryAdjuster.py\" , \"moduleName\" : \"dictionaryAdjuster\" , \"className\" : \"DictionaryAdjuster\" }, \"settings\" : {\"additionalFiles\" : [{\"path\":\"${General:jobDir}/data/DataVisSettings.json\" , \"parentName\":\"gaga2\"}] } }\r\n\r\n[TemplatesOut]\r\nmyOtherFancyTemplate2   = ${Job:scriptDir}/input2.txt\r\n\r\n[MySettings]\r\n\r\nexeCommand = echo \"Input1: \" ; cat ${TemplatesOut:myOtherFancyTemplate}; echo \"Input2: \" ; cat ${TemplatesOut:myOtherFancyTemplate2} ;\r\n\r\ndataType = grid\r\n...\r\n```\r\n\r\nAs can be seen, the variables (in the form ``${section:option}``) in both ``.ini`` files are interpolated, and can be used all over the place.\r\nAlso environment variables are possible, specified by the syntax ``ENV::Variable``.\r\n\r\nThe user is free to add arbitrary variable definitions and use those in other templates (any text based files such as ``.xml,.txt,.json`` etc.) for example the ``myOtherFancyTemplate`` or ``myOtherFancyTemplate2`` above.\r\nThe job generator uses ``.ini`` files because they allow comments and explanations which is pleasing. (That might possible change in the future to support also ``.json`` files)\r\nThe options under the ``Template`` section specifies the template files to be substituted, if no same option is present under the ``TemplateOut`` section which specifies the output file, the files are writte0 by default n to the folder ``${Job:scriptDir}``\r\n\r\nThe ``myOtherFancyTemplate2`` is a template where the string in ``{...}`` is parsed as JSON string and allows the user to specify a special template adjuster (substituter). The default one is the one given above, namely the ``DictionaryAdjuster`` in ``jobGenerators/dictionaryAdjuster.py`` which can be given a set of other ``\"additionalFiles\"`` (JSON files) which are parsed and added to the set of replacement strings. This means the ``${General:jobDir}/templates/input2.txt`` under the section can contain dictionary references, e.g. ``${gaga2:dic1:dic2:dic3:value}``, given in the json file ``${General:jobDir}/data/DataVisSettings.json`` and also references to variables in the ``.ini`` files.\r\n\r\nIf you look at the output folder ``examples/simple/cluster/Launch_MyDataVisualization.0``, you will find all configured scripts, bash scripts to launch the MPI job.\r\n\r\nOf course this is only a simple example. A more difficult example is provided by a an Image Correlation Task where each MPI process executes a tool pipeline consisting of some image processing and afterwards some image correlation, this job is explained in the following to give the user more insight how this tool works and how it can be extended.\r\n\r\n## Parallel Image Coorelation Job\r\nto be continued\r\n\r\n\r\n## Stuff to Remember:\r\n\r\n  Important stuff to be documented:\r\n  - ${Job:localDir} might only become a valid absolut path if fully expanded by Bash on a cluster (might contain Job specific variable such as $TMPDIR which are not clear at configuration time)\r\n    However, this is rarely used in practice and absolute paths to the local directory on each node on the cluster is suggested. When using env. variables inside ${Job:localDir}, do not store the value ${Job:localDir} in configuration files which are not Bash scripts (because env. variables do not get expanded)! If you need localDir hand it to your executable and deal with it.\r\n\r\n\r\n## Dependencies\r\npython 3, with modules:\r\n\r\n  - lxml\r\n  - glob2\r\n  - jsonpickle\r\n  - demjson\r\n  - AttrMap\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}